{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd248-807c-447d-aeac-58f1b69e99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_IN = 'route_based.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6c0b-796a-4618-b3fe-c04760eb863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re, json, subprocess, sys\n",
    "import importlib.util as il\n",
    "\n",
    "if None in [il.find_spec('python-ulid'), il.find_spec('pyperclip'), il.find_spec('pandas')]:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'python-ulid']);\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pyperclip']);\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pandas']);\n",
    "    \n",
    "from ulid import ULID\n",
    "import pyperclip\n",
    "import pandas as pd\n",
    "\n",
    "def gen_ulid():\n",
    "    return str(ULID.from_timestamp(time.time()))\n",
    "\n",
    "def convert_coord(c):\n",
    "    c = str(c)\n",
    "    j = len(c) - 6\n",
    "    d = int(c[0:2 + j])\n",
    "    m = int(c[2 + j:4 + j])\n",
    "    s = float(c[4 + j:6 + j] + '.' + c[6 + j:])\n",
    "    q = 1 if j == 0 else -1\n",
    "    coord = round(q * (d + m / 60 + s / 3600), 6)\n",
    "    \n",
    "    return coord\n",
    "\n",
    "def pprint(dict):\n",
    "    print(json.dumps(dict, indent=2))\n",
    "\n",
    "def comma_followed_by_number(s):\n",
    "    for i, char in enumerate(s[:-1]):\n",
    "        if char == ',' and s[i+1].isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_table_section_from_file(section_header, filename, offset=0):\n",
    "    offset *= 3\n",
    "    section_header = '******* ' + section_header + ' *******'\n",
    "\n",
    "    downloads_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    with open(os.path.join(downloads_folder, filename), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    extracted_lines = []\n",
    "    inside_section = False\n",
    "    end_marker_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if section_header in line:\n",
    "            inside_section = True\n",
    "            extracted_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        if inside_section:\n",
    "            if end_marker_count > offset:\n",
    "                extracted_lines.append(line)\n",
    "            # Count lines that are mostly dashes\n",
    "            if line.strip().startswith('---'):\n",
    "                end_marker_count += 1\n",
    "                if end_marker_count >= 3 + offset:\n",
    "                    break\n",
    "\n",
    "    return \"\".join(extracted_lines)\n",
    "\n",
    "def remove_dash_lines(text):\n",
    "    cleaned_lines = [\n",
    "        line for line in text.splitlines()\n",
    "        if not line.strip().startswith(\"---\")\n",
    "    ]\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def convert_pipe_text_to_csv(multi_line_text):\n",
    "    csv_lines = []\n",
    "    for line in multi_line_text.splitlines():\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if '|' not in line:\n",
    "            continue\n",
    "        \n",
    "        fields = [field.strip() for field in line.strip('|').split('|')]\n",
    "        csv_line = '|'.join(fields)\n",
    "        csv_lines.append(csv_line)\n",
    "\n",
    "    return '\\n'.join(csv_lines)\n",
    "\n",
    "def csv_text_to_dataframe(csv_text):\n",
    "    lines = [line.strip() for line in csv_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    headers = [h.strip() for h in lines[0].split('|')]\n",
    "    \n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        fields = [f.strip() for f in line.split('|')]\n",
    "        data.append(fields)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df\n",
    "\n",
    "def read_adaptation_section(section_header, filename, offset=0):\n",
    "    text = extract_table_section_from_file(section_header, filename, offset)\n",
    "    text = remove_dash_lines(text)\n",
    "    text = convert_pipe_text_to_csv(text)\n",
    "    \n",
    "    return csv_text_to_dataframe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539b395-a738-4802-b904-eb8903b58441",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "cols = [\"Arts Id\", \"Route Type\", \"Route Id\", \"Owning Facility\", \"Route Fix\",\n",
    "    \"ARTS Fix\", \"Unique Name\", \"Altitude Lower\", \"Altitude Upper\", \"Ac Class Criterias\"]\n",
    "df = pd.read_csv(os.path.join(downloads_path, FILE_IN), usecols=cols)\n",
    "\n",
    "facility = df[df['Arts Id'] == 'FFF']\n",
    "\n",
    "s = []\n",
    "dp_df = facility[(facility['Route Type'] == 'DP') | (facility['Route Type'] == 'AIRWAY')]\n",
    "for index, row in dp_df.iterrows():\n",
    "    if row['Route Type'] == 'DP':\n",
    "        route_id = re.sub(r'\\d+$', '#', row['Route Id'])\n",
    "    else:\n",
    "        route_id = row['Route Id']\n",
    "    data = route_id + (' ' + str(row['Route Fix'])).replace(' nan', '')\n",
    "    pattern = row['ARTS Fix']\n",
    "    \n",
    "    if not pd.isna(row['Altitude Lower']):\n",
    "        aoa = int(row['Altitude Lower'] / 100)\n",
    "        data += ' AOA/' + f\"{aoa:03d}\"\n",
    "    if not pd.isna(row['Altitude Upper']):\n",
    "        aob = int(row['Altitude Upper'] / 100)\n",
    "        data += ' AOB/' + f\"{aob:03d}\"\n",
    "    if not pd.isna(row['Ac Class Criterias']):\n",
    "        acc = row['Ac Class Criterias']\n",
    "        if not('NATJ' in acc or 'NATM' in acc or \\\n",
    "               'ZMAQ' in acc or 'ZMAP' in acc):\n",
    "            continue\n",
    "        \n",
    "        data += ' TYP/'\n",
    "        if 'NATJ' in acc:\n",
    "            data += 'J'\n",
    "        if 'NATM' in acc:\n",
    "            data += 'T'\n",
    "        if 'ZMAQ' in acc or 'ZMAP' in acc:\n",
    "            data += 'P'\n",
    "\n",
    "    out = 'fix_pattern[\\'' + data + '\\'] = \\'' + pattern + '\\'\\n'\n",
    "    if not out in s:\n",
    "        s.append(out)\n",
    "\n",
    "adr_df = facility[facility['Route Type'] == 'ADR']\n",
    "for index, row in adr_df.iterrows():\n",
    "    if pd.isna(row['Route Fix']):\n",
    "        continue\n",
    "    elif row['Route Fix'][3:] == 'WX':\n",
    "        continue\n",
    "    \n",
    "    data = row['Route Fix']\n",
    "    pattern = row['ARTS Fix']\n",
    "    \n",
    "    if not pd.isna(row['Altitude Lower']):\n",
    "        aoa = int(row['Altitude Lower'] / 100)\n",
    "        data += ' AOA/' + f\"{aoa:03d}\"\n",
    "    if not pd.isna(row['Altitude Upper']):\n",
    "        aob = int(row['Altitude Upper'] / 100)\n",
    "        data += ' AOB/' + f\"{aob:03d}\"\n",
    "    if not pd.isna(row['Ac Class Criterias']):\n",
    "        acc = row['Ac Class Criterias']\n",
    "        if not('NATJ' in acc or 'NATM' in acc or \\\n",
    "               'ZMAQ' in acc or 'ZMAP' in acc):\n",
    "            continue\n",
    "        \n",
    "        data += ' TYP/'\n",
    "        if 'NATJ' in acc:\n",
    "            data += 'J'\n",
    "        if 'NATM' in acc:\n",
    "            data += 'T'\n",
    "        if 'ZMAQ' in acc or 'ZMAP' in acc:\n",
    "            data += 'P'\n",
    "\n",
    "    out = 'fix_pattern[\\'' + data + '\\'] = \\'' + pattern + '\\'\\n'\n",
    "    if not out in s:\n",
    "        s.append(out)\n",
    "\n",
    "s_out = ''.join(sorted(s, key=lambda x: x.split(\"=\")[1]))\n",
    "pyperclip.copy(s_out)\n",
    "print(s_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b43f7-f8de-46eb-8927-9450be88f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_pattern = {}\n",
    "\n",
    "fix_pattern['AABER'] = 'AAB'\n",
    "fix_pattern['V599 THNDR AOA/051 AOB/999'] = 'BOD'\n",
    "fix_pattern['CHARO'] = 'CHR'\n",
    "fix_pattern['CSHEL'] = 'CPP'\n",
    "fix_pattern['PULEC'] = 'CPP'\n",
    "fix_pattern['CSHEL# CSHEL'] = 'CSH'\n",
    "fix_pattern['CSHEL'] = 'CSH'\n",
    "fix_pattern['V599 THNDR AOA/000 AOB/051'] = 'DBO'\n",
    "fix_pattern['DYLYN'] = 'DYL'\n",
    "fix_pattern['DYLYN'] = 'ELA'\n",
    "fix_pattern['MOOKY'] = 'GUL'\n",
    "fix_pattern['PIKKR AOA/000 AOB/101'] = 'GUL'\n",
    "fix_pattern['PIKKR AOA/101 AOB/990'] = 'GUL'\n",
    "fix_pattern['PIKKR AOA/000 AOB/100'] = 'GUL'\n",
    "fix_pattern['PIKKR AOA/101 AOB/999'] = 'GUL'\n",
    "fix_pattern['MOOKY AOA/000 AOB/100'] = 'GUL'\n",
    "fix_pattern['MOOKY AOA/101 AOB/999'] = 'GUL'\n",
    "fix_pattern['IMOCK'] = 'IMO'\n",
    "fix_pattern['V7 ROGAN AOA/000 AOB/100'] = 'LLL'\n",
    "fix_pattern['RIGOR AOA/101 AOB/999'] = 'MAR'\n",
    "fix_pattern['RIGOR AOA/000 AOB/101'] = 'MAR'\n",
    "fix_pattern['KARTR AOA/000 AOB/101'] = 'MAR'\n",
    "fix_pattern['KARTR AOA/101 AOB/990'] = 'MAR'\n",
    "fix_pattern['ROGAN AOA/100 AOB/999'] = 'R0G'\n",
    "fix_pattern['ROGAN'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/091 AOB/101'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/101 AOB/999'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/000 AOB/091'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/091 AOB/990'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/000 AOB/100'] = 'ROG'\n",
    "fix_pattern['ROGAN AOA/100 AOB/999'] = 'ROG'\n",
    "fix_pattern['LAL'] = 'ROG'\n",
    "fix_pattern['ORL'] = 'ROG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b538d0-330e-465d-996c-8ec431ad271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_fix_pattern(k):\n",
    "    if '#' in k.split(' ')[0]:\n",
    "        priority = 0\n",
    "    elif any(c.isdigit() for c in k.split(' ')[0]):\n",
    "        priority = 1\n",
    "    else:\n",
    "        priority = 2\n",
    "    return (priority, k)\n",
    "\n",
    "fix_pattern = dict(sorted(fix_pattern.items(), key=lambda item: sort_fix_pattern(item[0])))\n",
    "# pprint(fix_pattern)\n",
    "\n",
    "scratchpads = []\n",
    "for s in fix_pattern:\n",
    "    p = {}\n",
    "    p['id'] = gen_ulid()\n",
    "\n",
    "    sp = s\n",
    "    if ' AOA/' in s:\n",
    "        aoa = int(s.split(' AOA/')[1][0:3])\n",
    "        if aoa != 0:\n",
    "            p['minAltitude'] = aoa\n",
    "        sp = re.sub(r' AOA/\\d{3}', '', sp)\n",
    "    if ' AOB/' in s:\n",
    "        aob = int(s.split(' AOB/')[1][0:3])\n",
    "        if aob <= 999:\n",
    "            p['maxAltitude'] = aob\n",
    "        sp = re.sub(r' AOB/\\d{3}', '', sp)\n",
    "\n",
    "    if ' DEP/' in s:\n",
    "        p['airportIds'] = s.split(' DEP/')[1].split(' ')[0].split('/')\n",
    "        sp = re.sub(r' DEP(/\\w{1,})+', '', sp)\n",
    "    \n",
    "    p['searchPattern'] = sp\n",
    "    p['template'] = fix_pattern[s]\n",
    "    scratchpads.append(p)\n",
    "\n",
    "downloads_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "out_name = filename.replace('.csv', '') + '_scratchpads.json'\n",
    "with open(os.path.join(downloads_folder, out_name), \"w\") as file:\n",
    "    json.dump(scratchpads, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
